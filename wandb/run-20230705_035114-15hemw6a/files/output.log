[07/05 03:51:31 d2.engine.defaults]: Model:
OneFormer(
  (backbone): D2SwinTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=1536, out_features=4608, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=1536, out_features=4608, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0): TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
            (1): TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(150, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=134, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 133
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
relative_position_bias_table
[32m[07/05 03:51:31 oneformer.data.dataset_mappers.coco_unified_new_baseline_dataset_mapper]: [39m[COCOUnifiedNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024))]
[32m[07/05 03:51:54 oneformer.data.datasets.register_coco_panoptic_annos_semseg]: [39mLoading /home/bingxing2/gpuuser206/mmdetection/data/coco/annotations/instances_train2017.json takes 16.37 seconds.
[32m[07/05 03:51:54 oneformer.data.datasets.register_coco_panoptic_annos_semseg]: [39mLoaded 118287 images in COCO format from /home/bingxing2/gpuuser206/mmdetection/data/coco/annotations/instances_train2017.json
[07/05 03:52:03 d2.data.build]: Removed 1021 images with no usable annotations. 117266 images left.
[07/05 03:52:06 d2.data.build]: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
[36m|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
[36m|    person     | 257253       |   bicycle    | 7056         |      car      | 43533        |
[36m|  motorcycle   | 8654         |   airplane   | 5129         |      bus      | 6061         |
[36m|     train     | 4570         |    truck     | 9970         |     boat      | 10576        |
[36m| traffic light | 12842        | fire hydrant | 1865         |   stop sign   | 1983         |
[36m| parking meter | 1283         |    bench     | 9820         |     bird      | 10542        |
[36m|      cat      | 4766         |     dog      | 5500         |     horse     | 6567         |
[36m|     sheep     | 9223         |     cow      | 8014         |   elephant    | 5484         |
[36m|     bear      | 1294         |    zebra     | 5269         |    giraffe    | 5128         |
[36m|   backpack    | 8714         |   umbrella   | 11265        |    handbag    | 12342        |
[36m|      tie      | 6448         |   suitcase   | 6112         |    frisbee    | 2681         |
[36m|     skis      | 6623         |  snowboard   | 2681         |  sports ball  | 6299         |
[36m|     kite      | 8802         | baseball bat | 3273         | baseball gl.. | 3747         |
[36m|  skateboard   | 5536         |  surfboard   | 6095         | tennis racket | 4807         |
[36m|    bottle     | 24070        |  wine glass  | 7839         |      cup      | 20574        |
[36m|     fork      | 5474         |    knife     | 7760         |     spoon     | 6159         |
[36m|     bowl      | 14323        |    banana    | 9195         |     apple     | 5776         |
[36m|   sandwich    | 4356         |    orange    | 6302         |   broccoli    | 7261         |
[36m|    carrot     | 7758         |   hot dog    | 2884         |     pizza     | 5807         |
[36m|     donut     | 7005         |     cake     | 6296         |     chair     | 38073        |
[36m|     couch     | 5779         | potted plant | 8631         |      bed      | 4192         |
[36m| dining table  | 15695        |    toilet    | 4149         |      tv       | 5803         |
[36m|    laptop     | 4960         |    mouse     | 2261         |    remote     | 5700         |
[36m|   keyboard    | 2854         |  cell phone  | 6422         |   microwave   | 1672         |
[36m|     oven      | 3334         |   toaster    | 225          |     sink      | 5609         |
[36m| refrigerator  | 2634         |     book     | 24077        |     clock     | 6320         |
[36m|     vase      | 6577         |   scissors   | 1464         |  teddy bear   | 4729         |
[36m|  hair drier   | 198          |  toothbrush  | 1945         |               |              |
[36m|     total     | 849949       |              |              |               |              |
[07/05 03:52:06 d2.data.build]: Using training sampler TrainingSampler
[07/05 03:52:06 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[07/05 03:52:06 d2.data.common]: Serializing 117266 elements to byte tensors and concatenating them all ...
[07/05 03:52:11 d2.data.common]: Serialized dataset takes 540.14 MiB
[07/05 03:52:14 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./checkpoints/swin_large_patch4_window12_384_22kto1k.pkl ...
[07/05 03:52:14 fvcore.common.checkpoint]: [Checkpointer] Loading from ./checkpoints/swin_large_patch4_window12_384_22kto1k.pkl ...
[07/05 03:52:17 fvcore.common.checkpoint]: Reading a file from 'third_party'
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.adapter_1.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.adapter_1.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.layer_1.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.pixel_decoder.layer_1.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.class_transformer.decoder.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.class_transformer.decoder.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.bias in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.bias will not be loaded. Please double check and see if this is desired.
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: Shape of norm.weight in checkpoint is torch.Size([1536]), while shape of sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight in model is torch.Size([256]).
[31m[5mWARNING[39m[25m [07/05 03:52:17 d2.checkpoint.c2_model_loading]: norm.weight will not be loaded. Please double check and see if this is desired.
[07/05 03:52:17 d2.checkpoint.c2_model_loading]: Following weights matched with submodule backbone:
| Names in Model                       | Names in Checkpoint                                                                                                      | Shapes                                                     |
|:-------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------|
| layers.0.blocks.0.attn.*             | layers.0.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (192,) (192,192) (576,) (576,192) (529,6) (144,144)        |
| layers.0.blocks.0.mlp.fc1.*          | layers.0.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (768,) (768,192)                                           |
| layers.0.blocks.0.mlp.fc2.*          | layers.0.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (192,) (192,768)                                           |
| layers.0.blocks.0.norm1.*            | layers.0.blocks.0.norm1.{bias,weight}                                                                                    | (192,) (192,)                                              |
| layers.0.blocks.0.norm2.*            | layers.0.blocks.0.norm2.{bias,weight}                                                                                    | (192,) (192,)                                              |
| layers.0.blocks.1.attn.*             | layers.0.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (192,) (192,192) (576,) (576,192) (529,6) (144,144)        |
| layers.0.blocks.1.mlp.fc1.*          | layers.0.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (768,) (768,192)                                           |
| layers.0.blocks.1.mlp.fc2.*          | layers.0.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (192,) (192,768)                                           |
| layers.0.blocks.1.norm1.*            | layers.0.blocks.1.norm1.{bias,weight}                                                                                    | (192,) (192,)                                              |
| layers.0.blocks.1.norm2.*            | layers.0.blocks.1.norm2.{bias,weight}                                                                                    | (192,) (192,)                                              |
| layers.0.downsample.norm.*           | layers.0.downsample.norm.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.0.downsample.reduction.weight | layers.0.downsample.reduction.weight                                                                                     | (384, 768)                                                 |
| layers.1.blocks.0.attn.*             | layers.1.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (384,) (384,384) (1152,) (1152,384) (529,12) (144,144)     |
| layers.1.blocks.0.mlp.fc1.*          | layers.1.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (1536,) (1536,384)                                         |
| layers.1.blocks.0.mlp.fc2.*          | layers.1.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (384,) (384,1536)                                          |
| layers.1.blocks.0.norm1.*            | layers.1.blocks.0.norm1.{bias,weight}                                                                                    | (384,) (384,)                                              |
| layers.1.blocks.0.norm2.*            | layers.1.blocks.0.norm2.{bias,weight}                                                                                    | (384,) (384,)                                              |
| layers.1.blocks.1.attn.*             | layers.1.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (384,) (384,384) (1152,) (1152,384) (529,12) (144,144)     |
| layers.1.blocks.1.mlp.fc1.*          | layers.1.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (1536,) (1536,384)                                         |
| layers.1.blocks.1.mlp.fc2.*          | layers.1.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (384,) (384,1536)                                          |
| layers.1.blocks.1.norm1.*            | layers.1.blocks.1.norm1.{bias,weight}                                                                                    | (384,) (384,)                                              |
| layers.1.blocks.1.norm2.*            | layers.1.blocks.1.norm2.{bias,weight}                                                                                    | (384,) (384,)                                              |
| layers.1.downsample.norm.*           | layers.1.downsample.norm.{bias,weight}                                                                                   | (1536,) (1536,)                                            |
| layers.1.downsample.reduction.weight | layers.1.downsample.reduction.weight                                                                                     | (768, 1536)                                                |
| layers.2.blocks.0.attn.*             | layers.2.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.0.mlp.fc1.*          | layers.2.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.0.mlp.fc2.*          | layers.2.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.0.norm1.*            | layers.2.blocks.0.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.0.norm2.*            | layers.2.blocks.0.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.1.attn.*             | layers.2.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.1.mlp.fc1.*          | layers.2.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.1.mlp.fc2.*          | layers.2.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.1.norm1.*            | layers.2.blocks.1.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.1.norm2.*            | layers.2.blocks.1.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.10.attn.*            | layers.2.blocks.10.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.10.mlp.fc1.*         | layers.2.blocks.10.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.10.mlp.fc2.*         | layers.2.blocks.10.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.10.norm1.*           | layers.2.blocks.10.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.10.norm2.*           | layers.2.blocks.10.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.11.attn.*            | layers.2.blocks.11.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.11.mlp.fc1.*         | layers.2.blocks.11.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.11.mlp.fc2.*         | layers.2.blocks.11.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.11.norm1.*           | layers.2.blocks.11.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.11.norm2.*           | layers.2.blocks.11.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.12.attn.*            | layers.2.blocks.12.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.12.mlp.fc1.*         | layers.2.blocks.12.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.12.mlp.fc2.*         | layers.2.blocks.12.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.12.norm1.*           | layers.2.blocks.12.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.12.norm2.*           | layers.2.blocks.12.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.13.attn.*            | layers.2.blocks.13.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.13.mlp.fc1.*         | layers.2.blocks.13.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.13.mlp.fc2.*         | layers.2.blocks.13.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.13.norm1.*           | layers.2.blocks.13.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.13.norm2.*           | layers.2.blocks.13.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.14.attn.*            | layers.2.blocks.14.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.14.mlp.fc1.*         | layers.2.blocks.14.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.14.mlp.fc2.*         | layers.2.blocks.14.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.14.norm1.*           | layers.2.blocks.14.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.14.norm2.*           | layers.2.blocks.14.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.15.attn.*            | layers.2.blocks.15.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.15.mlp.fc1.*         | layers.2.blocks.15.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.15.mlp.fc2.*         | layers.2.blocks.15.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.15.norm1.*           | layers.2.blocks.15.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.15.norm2.*           | layers.2.blocks.15.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.16.attn.*            | layers.2.blocks.16.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.16.mlp.fc1.*         | layers.2.blocks.16.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.16.mlp.fc2.*         | layers.2.blocks.16.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.16.norm1.*           | layers.2.blocks.16.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.16.norm2.*           | layers.2.blocks.16.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.17.attn.*            | layers.2.blocks.17.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index} | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.17.mlp.fc1.*         | layers.2.blocks.17.mlp.fc1.{bias,weight}                                                                                 | (3072,) (3072,768)                                         |
| layers.2.blocks.17.mlp.fc2.*         | layers.2.blocks.17.mlp.fc2.{bias,weight}                                                                                 | (768,) (768,3072)                                          |
| layers.2.blocks.17.norm1.*           | layers.2.blocks.17.norm1.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.17.norm2.*           | layers.2.blocks.17.norm2.{bias,weight}                                                                                   | (768,) (768,)                                              |
| layers.2.blocks.2.attn.*             | layers.2.blocks.2.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.2.mlp.fc1.*          | layers.2.blocks.2.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.2.mlp.fc2.*          | layers.2.blocks.2.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.2.norm1.*            | layers.2.blocks.2.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.2.norm2.*            | layers.2.blocks.2.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.3.attn.*             | layers.2.blocks.3.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.3.mlp.fc1.*          | layers.2.blocks.3.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.3.mlp.fc2.*          | layers.2.blocks.3.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.3.norm1.*            | layers.2.blocks.3.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.3.norm2.*            | layers.2.blocks.3.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.4.attn.*             | layers.2.blocks.4.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.4.mlp.fc1.*          | layers.2.blocks.4.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.4.mlp.fc2.*          | layers.2.blocks.4.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.4.norm1.*            | layers.2.blocks.4.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.4.norm2.*            | layers.2.blocks.4.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.5.attn.*             | layers.2.blocks.5.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.5.mlp.fc1.*          | layers.2.blocks.5.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.5.mlp.fc2.*          | layers.2.blocks.5.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.5.norm1.*            | layers.2.blocks.5.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.5.norm2.*            | layers.2.blocks.5.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.6.attn.*             | layers.2.blocks.6.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.6.mlp.fc1.*          | layers.2.blocks.6.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.6.mlp.fc2.*          | layers.2.blocks.6.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.6.norm1.*            | layers.2.blocks.6.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.6.norm2.*            | layers.2.blocks.6.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.7.attn.*             | layers.2.blocks.7.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.7.mlp.fc1.*          | layers.2.blocks.7.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.7.mlp.fc2.*          | layers.2.blocks.7.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.7.norm1.*            | layers.2.blocks.7.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.7.norm2.*            | layers.2.blocks.7.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.8.attn.*             | layers.2.blocks.8.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.8.mlp.fc1.*          | layers.2.blocks.8.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.8.mlp.fc2.*          | layers.2.blocks.8.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.8.norm1.*            | layers.2.blocks.8.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.8.norm2.*            | layers.2.blocks.8.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.9.attn.*             | layers.2.blocks.9.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (768,) (768,768) (2304,) (2304,768) (529,24) (144,144)     |
| layers.2.blocks.9.mlp.fc1.*          | layers.2.blocks.9.mlp.fc1.{bias,weight}                                                                                  | (3072,) (3072,768)                                         |
| layers.2.blocks.9.mlp.fc2.*          | layers.2.blocks.9.mlp.fc2.{bias,weight}                                                                                  | (768,) (768,3072)                                          |
| layers.2.blocks.9.norm1.*            | layers.2.blocks.9.norm1.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.blocks.9.norm2.*            | layers.2.blocks.9.norm2.{bias,weight}                                                                                    | (768,) (768,)                                              |
| layers.2.downsample.norm.*           | layers.2.downsample.norm.{bias,weight}                                                                                   | (3072,) (3072,)                                            |
| layers.2.downsample.reduction.weight | layers.2.downsample.reduction.weight                                                                                     | (1536, 3072)                                               |
| layers.3.blocks.0.attn.*             | layers.3.blocks.0.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (1536,) (1536,1536) (4608,) (4608,1536) (529,48) (144,144) |
| layers.3.blocks.0.mlp.fc1.*          | layers.3.blocks.0.mlp.fc1.{bias,weight}                                                                                  | (6144,) (6144,1536)                                        |
| layers.3.blocks.0.mlp.fc2.*          | layers.3.blocks.0.mlp.fc2.{bias,weight}                                                                                  | (1536,) (1536,6144)                                        |
| layers.3.blocks.0.norm1.*            | layers.3.blocks.0.norm1.{bias,weight}                                                                                    | (1536,) (1536,)                                            |
| layers.3.blocks.0.norm2.*            | layers.3.blocks.0.norm2.{bias,weight}                                                                                    | (1536,) (1536,)                                            |
| layers.3.blocks.1.attn.*             | layers.3.blocks.1.attn.{proj.bias,proj.weight,qkv.bias,qkv.weight,relative_position_bias_table,relative_position_index}  | (1536,) (1536,1536) (4608,) (4608,1536) (529,48) (144,144) |
| layers.3.blocks.1.mlp.fc1.*          | layers.3.blocks.1.mlp.fc1.{bias,weight}                                                                                  | (6144,) (6144,1536)                                        |
| layers.3.blocks.1.mlp.fc2.*          | layers.3.blocks.1.mlp.fc2.{bias,weight}                                                                                  | (1536,) (1536,6144)                                        |
| layers.3.blocks.1.norm1.*            | layers.3.blocks.1.norm1.{bias,weight}                                                                                    | (1536,) (1536,)                                            |
| layers.3.blocks.1.norm2.*            | layers.3.blocks.1.norm2.{bias,weight}                                                                                    | (1536,) (1536,)                                            |
| patch_embed.norm.*                   | patch_embed.norm.{bias,weight}                                                                                           | (192,) (192,)                                              |
| patch_embed.proj.*                   | patch_embed.proj.{bias,weight}                                                                                           | (192,) (192,3,4,4)                                         |
[31m[5mWARNING[39m[25m [07/05 03:52:18 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.norm0.{bias, weight}
[34mbackbone.norm1.{bias, weight}
[34mbackbone.norm2.{bias, weight}
[34mbackbone.norm3.{bias, weight}
[34mcriterion.{empty_weight, logit_scale}
[34mprompt_ctx.weight
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}
[34msem_seg_head.pixel_decoder.adapter_1.weight
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}
[34msem_seg_head.pixel_decoder.layer_1.weight
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.level_embed
[34msem_seg_head.predictor.class_embed.{bias, weight}
[34msem_seg_head.predictor.class_input_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.linear1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.linear2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.norm1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.norm2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.norm3.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.linear1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.linear2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.norm1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.norm2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.norm3.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.norm.{bias, weight}
[34msem_seg_head.predictor.decoder_norm.{bias, weight}
[34msem_seg_head.predictor.level_embed.weight
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}
[34msem_seg_head.predictor.query_embed.weight
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}
[34mtask_mlp.layers.0.{bias, weight}
[34mtask_mlp.layers.1.{bias, weight}
[34mtext_encoder.ln_final.{bias, weight}
[34mtext_encoder.positional_embedding
[34mtext_encoder.token_embedding.weight
[34mtext_encoder.transformer.resblocks.0.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.0.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.1.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.2.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.3.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.4.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.5.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.mlp.c_proj.{bias, weight}
[34mtext_projector.layers.0.{bias, weight}
[34mtext_projector.layers.1.{bias, weight}
[31m[5mWARNING[39m[25m [07/05 03:52:18 fvcore.common.checkpoint]: The checkpoint state_dict contains keys that are not used by the model:
  [35mhead.{bias, weight}
  [35mlayers.0.blocks.1.attn_mask
  [35mlayers.1.blocks.1.attn_mask
  [35mlayers.2.blocks.1.attn_mask
  [35mlayers.2.blocks.11.attn_mask
  [35mlayers.2.blocks.13.attn_mask
  [35mlayers.2.blocks.15.attn_mask
  [35mlayers.2.blocks.17.attn_mask
  [35mlayers.2.blocks.3.attn_mask
  [35mlayers.2.blocks.5.attn_mask
  [35mlayers.2.blocks.7.attn_mask
  [35mlayers.2.blocks.9.attn_mask
  [35mnorm.{bias, weight}
Total Params: 236.324091 M
[07/05 03:52:21 d2.engine.train_loop]: Starting training from iteration 0
/home/bingxing2/gpuuser206/.conda/envs/oneformer/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/bingxing2/gpuuser206/.conda/envs/oneformer/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/bingxing2/gpuuser206/.conda/envs/oneformer/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/bingxing2/gpuuser206/.conda/envs/oneformer/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/bingxing2/gpuuser206/.conda/envs/oneformer/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[07/05 03:53:27 d2.utils.events]:  eta: 12 days, 14:46:40  iter: 19  total_loss: 105.9  loss_ce: 4.479  loss_mask: 1.388  loss_dice: 4.673  loss_contrastive: 1.538  loss_ce_0: 4.517  loss_mask_0: 1.242  loss_dice_0: 4.595  loss_ce_1: 4.493  loss_mask_1: 1.221  loss_dice_1: 4.619  loss_ce_2: 4.401  loss_mask_2: 1.324  loss_dice_2: 4.658  loss_ce_3: 4.437  loss_mask_3: 1.366  loss_dice_3: 4.698  loss_ce_4: 4.422  loss_mask_4: 1.431  loss_dice_4: 4.669  loss_ce_5: 4.47  loss_mask_5: 1.431  loss_dice_5: 4.704  loss_ce_6: 4.352  loss_mask_6: 1.407  loss_dice_6: 4.698  loss_ce_7: 4.448  loss_mask_7: 1.438  loss_dice_7: 4.685  loss_ce_8: 4.433  loss_mask_8: 1.457  loss_dice_8: 4.677    time: 1.4870  last_time: 1.4839  data_time: 1.4270  last_data_time: 0.0237   lr: 0.0001  max_mem: 23407M
[07/05 03:53:53 d2.utils.events]:  eta: 12 days, 14:34:24  iter: 39  total_loss: 100.2  loss_ce: 3.903  loss_mask: 1.482  loss_dice: 4.597  loss_contrastive: 1.279  loss_ce_0: 3.918  loss_mask_0: 1.184  loss_dice_0: 4.45  loss_ce_1: 3.885  loss_mask_1: 1.13  loss_dice_1: 4.473  loss_ce_2: 3.904  loss_mask_2: 1.181  loss_dice_2: 4.502  loss_ce_3: 3.848  loss_mask_3: 1.352  loss_dice_3: 4.546  loss_ce_4: 3.89  loss_mask_4: 1.435  loss_dice_4: 4.597  loss_ce_5: 3.899  loss_mask_5: 1.455  loss_dice_5: 4.601  loss_ce_6: 3.874  loss_mask_6: 1.495  loss_dice_6: 4.6  loss_ce_7: 3.884  loss_mask_7: 1.447  loss_dice_7: 4.6  loss_ce_8: 3.908  loss_mask_8: 1.457  loss_dice_8: 4.611    time: 1.4104  last_time: 0.7911  data_time: 0.0470  last_data_time: 0.0775   lr: 0.0001  max_mem: 23568M
[07/05 03:54:18 d2.utils.events]:  eta: 12 days, 14:11:14  iter: 59  total_loss: 98.15  loss_ce: 3.851  loss_mask: 1.337  loss_dice: 4.651  loss_contrastive: 1.14  loss_ce_0: 3.826  loss_mask_0: 1.108  loss_dice_0: 4.504  loss_ce_1: 3.745  loss_mask_1: 1.128  loss_dice_1: 4.539  loss_ce_2: 3.752  loss_mask_2: 1.144  loss_dice_2: 4.552  loss_ce_3: 3.802  loss_mask_3: 1.151  loss_dice_3: 4.576  loss_ce_4: 3.847  loss_mask_4: 1.209  loss_dice_4: 4.602  loss_ce_5: 3.859  loss_mask_5: 1.317  loss_dice_5: 4.641  loss_ce_6: 3.838  loss_mask_6: 1.307  loss_dice_6: 4.645  loss_ce_7: 3.854  loss_mask_7: 1.306  loss_dice_7: 4.658  loss_ce_8: 3.855  loss_mask_8: 1.309  loss_dice_8: 4.652    time: 1.3539  last_time: 0.7896  data_time: 0.0485  last_data_time: 0.0167   lr: 0.0001  max_mem: 23568M
[07/05 03:54:39 d2.utils.events]:  eta: 12 days, 12:25:22  iter: 79  total_loss: 98.86  loss_ce: 3.946  loss_mask: 1.418  loss_dice: 4.602  loss_contrastive: 1.063  loss_ce_0: 3.935  loss_mask_0: 1.086  loss_dice_0: 4.396  loss_ce_1: 3.886  loss_mask_1: 1.133  loss_dice_1: 4.456  loss_ce_2: 3.897  loss_mask_2: 1.154  loss_dice_2: 4.489  loss_ce_3: 3.904  loss_mask_3: 1.162  loss_dice_3: 4.523  loss_ce_4: 3.934  loss_mask_4: 1.166  loss_dice_4: 4.509  loss_ce_5: 3.958  loss_mask_5: 1.277  loss_dice_5: 4.565  loss_ce_6: 3.939  loss_mask_6: 1.382  loss_dice_6: 4.581  loss_ce_7: 3.945  loss_mask_7: 1.43  loss_dice_7: 4.6  loss_ce_8: 3.964  loss_mask_8: 1.45  loss_dice_8: 4.616    time: 1.2719  last_time: 0.7021  data_time: 0.0470  last_data_time: 0.0623   lr: 0.0001  max_mem: 23568M
[07/05 03:54:59 d2.utils.events]:  eta: 12 days, 9:19:18  iter: 99  total_loss: 96.13  loss_ce: 3.79  loss_mask: 1.311  loss_dice: 4.642  loss_contrastive: 1.057  loss_ce_0: 3.789  loss_mask_0: 1.001  loss_dice_0: 4.441  loss_ce_1: 3.739  loss_mask_1: 0.9958  loss_dice_1: 4.454  loss_ce_2: 3.717  loss_mask_2: 1.031  loss_dice_2: 4.489  loss_ce_3: 3.742  loss_mask_3: 1.06  loss_dice_3: 4.511  loss_ce_4: 3.731  loss_mask_4: 1.039  loss_dice_4: 4.538  loss_ce_5: 3.765  loss_mask_5: 1.092  loss_dice_5: 4.551  loss_ce_6: 3.773  loss_mask_6: 1.136  loss_dice_6: 4.574  loss_ce_7: 3.796  loss_mask_7: 1.276  loss_dice_7: 4.618  loss_ce_8: 3.76  loss_mask_8: 1.329  loss_dice_8: 4.636    time: 1.2089  last_time: 0.7800  data_time: 0.0449  last_data_time: 0.0198   lr: 0.0001  max_mem: 23568M
[07/05 03:55:15 d2.utils.events]:  eta: 11 days, 19:30:07  iter: 119  total_loss: 93.97  loss_ce: 3.91  loss_mask: 1.225  loss_dice: 4.603  loss_contrastive: 1.133  loss_ce_0: 3.884  loss_mask_0: 1.013  loss_dice_0: 4.358  loss_ce_1: 3.768  loss_mask_1: 0.9879  loss_dice_1: 4.402  loss_ce_2: 3.767  loss_mask_2: 0.9749  loss_dice_2: 4.432  loss_ce_3: 3.768  loss_mask_3: 0.9833  loss_dice_3: 4.413  loss_ce_4: 3.676  loss_mask_4: 0.9587  loss_dice_4: 4.327  loss_ce_5: 3.679  loss_mask_5: 0.9584  loss_dice_5: 4.357  loss_ce_6: 3.784  loss_mask_6: 0.9819  loss_dice_6: 4.393  loss_ce_7: 3.865  loss_mask_7: 1.016  loss_dice_7: 4.484  loss_ce_8: 3.884  loss_mask_8: 1.119  loss_dice_8: 4.585    time: 1.1424  last_time: 0.8832  data_time: 0.0418  last_data_time: 0.0828   lr: 0.0001  max_mem: 23568M
[07/05 03:55:30 d2.utils.events]:  eta: 7 days, 4:57:09  iter: 139  total_loss: 89.13  loss_ce: 3.837  loss_mask: 1.082  loss_dice: 4.398  loss_contrastive: 1.069  loss_ce_0: 3.81  loss_mask_0: 0.9816  loss_dice_0: 4.229  loss_ce_1: 3.747  loss_mask_1: 0.9903  loss_dice_1: 4.166  loss_ce_2: 3.791  loss_mask_2: 0.9894  loss_dice_2: 4.195  loss_ce_3: 3.748  loss_mask_3: 0.9749  loss_dice_3: 4.143  loss_ce_4: 3.634  loss_mask_4: 0.9835  loss_dice_4: 4.182  loss_ce_5: 3.676  loss_mask_5: 0.9631  loss_dice_5: 4.169  loss_ce_6: 3.573  loss_mask_6: 0.9404  loss_dice_6: 4.179  loss_ce_7: 3.717  loss_mask_7: 0.9604  loss_dice_7: 4.155  loss_ce_8: 3.824  loss_mask_8: 0.9499  loss_dice_8: 4.297    time: 1.0885  last_time: 0.7435  data_time: 0.0479  last_data_time: 0.0563   lr: 0.0001  max_mem: 23568M
[07/05 03:55:46 d2.utils.events]:  eta: 6 days, 21:34:23  iter: 159  total_loss: 88.65  loss_ce: 3.578  loss_mask: 0.8782  loss_dice: 4.389  loss_contrastive: 1.126  loss_ce_0: 3.715  loss_mask_0: 0.9348  loss_dice_0: 4.325  loss_ce_1: 3.689  loss_mask_1: 0.8884  loss_dice_1: 4.281  loss_ce_2: 3.67  loss_mask_2: 0.921  loss_dice_2: 4.256  loss_ce_3: 3.572  loss_mask_3: 0.9139  loss_dice_3: 4.259  loss_ce_4: 3.59  loss_mask_4: 0.905  loss_dice_4: 4.331  loss_ce_5: 3.475  loss_mask_5: 0.9065  loss_dice_5: 4.341  loss_ce_6: 3.45  loss_mask_6: 0.9087  loss_dice_6: 4.352  loss_ce_7: 3.422  loss_mask_7: 0.9163  loss_dice_7: 4.343  loss_ce_8: 3.485  loss_mask_8: 0.8872  loss_dice_8: 4.337    time: 1.0497  last_time: 0.7515  data_time: 0.0513  last_data_time: 0.0782   lr: 0.0001  max_mem: 23568M
[07/05 03:56:02 d2.utils.events]:  eta: 6 days, 19:19:34  iter: 179  total_loss: 89.07  loss_ce: 3.557  loss_mask: 0.9918  loss_dice: 4.253  loss_contrastive: 1.107  loss_ce_0: 3.698  loss_mask_0: 0.9973  loss_dice_0: 4.197  loss_ce_1: 3.543  loss_mask_1: 1.018  loss_dice_1: 4.114  loss_ce_2: 3.527  loss_mask_2: 1.041  loss_dice_2: 4.064  loss_ce_3: 3.458  loss_mask_3: 1.021  loss_dice_3: 4.081  loss_ce_4: 3.406  loss_mask_4: 1.021  loss_dice_4: 4.186  loss_ce_5: 3.387  loss_mask_5: 1.017  loss_dice_5: 4.203  loss_ce_6: 3.345  loss_mask_6: 1.006  loss_dice_6: 4.219  loss_ce_7: 3.387  loss_mask_7: 1.003  loss_dice_7: 4.199  loss_ce_8: 3.503  loss_mask_8: 0.992  loss_dice_8: 4.196    time: 1.0189  last_time: 0.7863  data_time: 0.0489  last_data_time: 0.0411   lr: 0.0001  max_mem: 23568M
[07/05 03:56:18 d2.utils.events]:  eta: 6 days, 18:54:05  iter: 199  total_loss: 85.97  loss_ce: 3.335  loss_mask: 1.048  loss_dice: 4.096  loss_contrastive: 1.099  loss_ce_0: 3.573  loss_mask_0: 0.9795  loss_dice_0: 4.131  loss_ce_1: 3.312  loss_mask_1: 1.004  loss_dice_1: 4.043  loss_ce_2: 3.346  loss_mask_2: 1.003  loss_dice_2: 4.018  loss_ce_3: 3.382  loss_mask_3: 1.049  loss_dice_3: 3.959  loss_ce_4: 3.378  loss_mask_4: 0.9808  loss_dice_4: 3.943  loss_ce_5: 3.348  loss_mask_5: 1.013  loss_dice_5: 3.981  loss_ce_6: 3.423  loss_mask_6: 1.06  loss_dice_6: 3.977  loss_ce_7: 3.425  loss_mask_7: 1.032  loss_dice_7: 3.947  loss_ce_8: 3.318  loss_mask_8: 1.025  loss_dice_8: 4.046    time: 0.9948  last_time: 0.7708  data_time: 0.0559  last_data_time: 0.0394   lr: 0.0001  max_mem: 23568M
[07/05 03:56:33 d2.utils.events]:  eta: 6 days, 17:48:35  iter: 219  total_loss: 86.32  loss_ce: 3.496  loss_mask: 0.942  loss_dice: 4.023  loss_contrastive: 1.12  loss_ce_0: 3.594  loss_mask_0: 0.9388  loss_dice_0: 4.115  loss_ce_1: 3.575  loss_mask_1: 0.9416  loss_dice_1: 3.992  loss_ce_2: 3.398  loss_mask_2: 0.9502  loss_dice_2: 4.012  loss_ce_3: 3.433  loss_mask_3: 0.9674  loss_dice_3: 3.95  loss_ce_4: 3.446  loss_mask_4: 0.9486  loss_dice_4: 3.92  loss_ce_5: 3.417  loss_mask_5: 0.999  loss_dice_5: 3.977  loss_ce_6: 3.589  loss_mask_6: 1.016  loss_dice_6: 3.951  loss_ce_7: 3.49  loss_mask_7: 0.9985  loss_dice_7: 3.966  loss_ce_8: 3.561  loss_mask_8: 0.9847  loss_dice_8: 3.982    time: 0.9737  last_time: 0.7703  data_time: 0.0393  last_data_time: 0.0163   lr: 0.0001  max_mem: 23568M
[07/05 03:56:49 d2.utils.events]:  eta: 6 days, 17:26:27  iter: 239  total_loss: 86.75  loss_ce: 3.672  loss_mask: 0.9433  loss_dice: 3.883  loss_contrastive: 1.123  loss_ce_0: 3.829  loss_mask_0: 0.9398  loss_dice_0: 4.14  loss_ce_1: 3.682  loss_mask_1: 0.9247  loss_dice_1: 3.847  loss_ce_2: 3.686  loss_mask_2: 0.9333  loss_dice_2: 3.857  loss_ce_3: 3.729  loss_mask_3: 0.9389  loss_dice_3: 3.767  loss_ce_4: 3.717  loss_mask_4: 0.9106  loss_dice_4: 3.784  loss_ce_5: 3.694  loss_mask_5: 0.9302  loss_dice_5: 3.806  loss_ce_6: 3.765  loss_mask_6: 0.9196  loss_dice_6: 3.805  loss_ce_7: 3.766  loss_mask_7: 0.9149  loss_dice_7: 3.835  loss_ce_8: 3.67  loss_mask_8: 0.9158  loss_dice_8: 3.81    time: 0.9574  last_time: 0.7971  data_time: 0.0482  last_data_time: 0.0216   lr: 0.0001  max_mem: 23568M
[07/05 03:57:04 d2.utils.events]:  eta: 6 days, 16:55:35  iter: 259  total_loss: 86.46  loss_ce: 3.641  loss_mask: 1.052  loss_dice: 3.668  loss_contrastive: 1.094  loss_ce_0: 3.812  loss_mask_0: 1.044  loss_dice_0: 3.875  loss_ce_1: 3.863  loss_mask_1: 1.046  loss_dice_1: 3.669  loss_ce_2: 3.896  loss_mask_2: 1.047  loss_dice_2: 3.647  loss_ce_3: 3.699  loss_mask_3: 1.04  loss_dice_3: 3.66  loss_ce_4: 3.683  loss_mask_4: 1.021  loss_dice_4: 3.648  loss_ce_5: 3.695  loss_mask_5: 1.053  loss_dice_5: 3.655  loss_ce_6: 3.572  loss_mask_6: 1.049  loss_dice_6: 3.661  loss_ce_7: 3.682  loss_mask_7: 1.051  loss_dice_7: 3.685  loss_ce_8: 3.555  loss_mask_8: 1.06  loss_dice_8: 3.667    time: 0.9431  last_time: 0.6914  data_time: 0.0425  last_data_time: 0.0792   lr: 0.0001  max_mem: 23568M
[07/05 03:57:20 d2.utils.events]:  eta: 6 days, 16:49:45  iter: 279  total_loss: 83.07  loss_ce: 3.712  loss_mask: 0.996  loss_dice: 3.532  loss_contrastive: 1.143  loss_ce_0: 3.868  loss_mask_0: 0.9782  loss_dice_0: 3.895  loss_ce_1: 3.788  loss_mask_1: 1.06  loss_dice_1: 3.599  loss_ce_2: 3.764  loss_mask_2: 1.002  loss_dice_2: 3.603  loss_ce_3: 3.717  loss_mask_3: 0.991  loss_dice_3: 3.48  loss_ce_4: 3.757  loss_mask_4: 1.016  loss_dice_4: 3.464  loss_ce_5: 3.852  loss_mask_5: 1.015  loss_dice_5: 3.483  loss_ce_6: 3.596  loss_mask_6: 1.001  loss_dice_6: 3.479  loss_ce_7: 3.624  loss_mask_7: 1  loss_dice_7: 3.573  loss_ce_8: 3.689  loss_mask_8: 1.01  loss_dice_8: 3.489    time: 0.9305  last_time: 0.7622  data_time: 0.0423  last_data_time: 0.0271   lr: 0.0001  max_mem: 23568M
[07/05 03:57:36 d2.utils.events]:  eta: 6 days, 16:35:57  iter: 299  total_loss: 81.74  loss_ce: 3.538  loss_mask: 1.151  loss_dice: 3.36  loss_contrastive: 1.122  loss_ce_0: 3.655  loss_mask_0: 1.065  loss_dice_0: 3.704  loss_ce_1: 3.548  loss_mask_1: 1.079  loss_dice_1: 3.483  loss_ce_2: 3.527  loss_mask_2: 1.11  loss_dice_2: 3.524  loss_ce_3: 3.47  loss_mask_3: 1.04  loss_dice_3: 3.296  loss_ce_4: 3.574  loss_mask_4: 1.054  loss_dice_4: 3.338  loss_ce_5: 3.689  loss_mask_5: 1.004  loss_dice_5: 3.41  loss_ce_6: 3.56  loss_mask_6: 1.038  loss_dice_6: 3.292  loss_ce_7: 3.586  loss_mask_7: 1.065  loss_dice_7: 3.278  loss_ce_8: 3.616  loss_mask_8: 1.049  loss_dice_8: 3.274    time: 0.9204  last_time: 0.8048  data_time: 0.0400  last_data_time: 0.0798   lr: 0.0001  max_mem: 23568M
[07/05 03:57:51 d2.utils.events]:  eta: 6 days, 16:20:20  iter: 319  total_loss: 81.28  loss_ce: 3.631  loss_mask: 0.9829  loss_dice: 3.417  loss_contrastive: 1.121  loss_ce_0: 3.881  loss_mask_0: 0.9288  loss_dice_0: 3.701  loss_ce_1: 3.711  loss_mask_1: 0.9465  loss_dice_1: 3.496  loss_ce_2: 3.664  loss_mask_2: 0.928  loss_dice_2: 3.456  loss_ce_3: 3.65  loss_mask_3: 0.9915  loss_dice_3: 3.379  loss_ce_4: 3.742  loss_mask_4: 0.9916  loss_dice_4: 3.282  loss_ce_5: 3.838  loss_mask_5: 0.9968  loss_dice_5: 3.25  loss_ce_6: 3.669  loss_mask_6: 0.9531  loss_dice_6: 3.301  loss_ce_7: 3.687  loss_mask_7: 0.9617  loss_dice_7: 3.311  loss_ce_8: 3.666  loss_mask_8: 0.991  loss_dice_8: 3.315    time: 0.9115  last_time: 0.7591  data_time: 0.0487  last_data_time: 0.0632   lr: 0.0001  max_mem: 23568M
[07/05 03:58:07 d2.utils.events]:  eta: 6 days, 15:59:52  iter: 339  total_loss: 82.36  loss_ce: 3.812  loss_mask: 0.9145  loss_dice: 3.395  loss_contrastive: 1.123  loss_ce_0: 3.987  loss_mask_0: 0.9798  loss_dice_0: 3.73  loss_ce_1: 3.769  loss_mask_1: 0.9525  loss_dice_1: 3.451  loss_ce_2: 3.8  loss_mask_2: 0.9761  loss_dice_2: 3.416  loss_ce_3: 3.746  loss_mask_3: 0.9385  loss_dice_3: 3.359  loss_ce_4: 3.787  loss_mask_4: 0.977  loss_dice_4: 3.393  loss_ce_5: 3.843  loss_mask_5: 0.9991  loss_dice_5: 3.293  loss_ce_6: 3.676  loss_mask_6: 0.9907  loss_dice_6: 3.374  loss_ce_7: 3.658  loss_mask_7: 1.011  loss_dice_7: 3.364  loss_ce_8: 3.674  loss_mask_8: 1.003  loss_dice_8: 3.383    time: 0.9030  last_time: 0.7576  data_time: 0.0433  last_data_time: 0.0261   lr: 0.0001  max_mem: 23568M
[07/05 03:58:22 d2.utils.events]:  eta: 6 days, 15:46:12  iter: 359  total_loss: 76.31  loss_ce: 3.336  loss_mask: 0.9698  loss_dice: 3.142  loss_contrastive: 1.087  loss_ce_0: 3.726  loss_mask_0: 0.9449  loss_dice_0: 3.585  loss_ce_1: 3.434  loss_mask_1: 0.9339  loss_dice_1: 3.227  loss_ce_2: 3.407  loss_mask_2: 0.904  loss_dice_2: 3.302  loss_ce_3: 3.479  loss_mask_3: 0.8864  loss_dice_3: 3.178  loss_ce_4: 3.476  loss_mask_4: 0.9371  loss_dice_4: 3.1  loss_ce_5: 3.526  loss_mask_5: 0.9296  loss_dice_5: 3.144  loss_ce_6: 3.389  loss_mask_6: 0.9116  loss_dice_6: 3.132  loss_ce_7: 3.404  loss_mask_7: 0.9501  loss_dice_7: 3.107  loss_ce_8: 3.46  loss_mask_8: 0.9468  loss_dice_8: 3.11    time: 0.8953  last_time: 0.6813  data_time: 0.0441  last_data_time: 0.0152   lr: 0.0001  max_mem: 23568M
[07/05 03:58:38 d2.utils.events]:  eta: 6 days, 15:40:53  iter: 379  total_loss: 78.76  loss_ce: 3.4  loss_mask: 0.8346  loss_dice: 3.261  loss_contrastive: 1.059  loss_ce_0: 3.681  loss_mask_0: 0.8593  loss_dice_0: 3.643  loss_ce_1: 3.503  loss_mask_1: 0.8642  loss_dice_1: 3.4  loss_ce_2: 3.504  loss_mask_2: 0.8122  loss_dice_2: 3.324  loss_ce_3: 3.479  loss_mask_3: 0.8398  loss_dice_3: 3.336  loss_ce_4: 3.478  loss_mask_4: 0.8475  loss_dice_4: 3.25  loss_ce_5: 3.503  loss_mask_5: 0.8248  loss_dice_5: 3.267  loss_ce_6: 3.363  loss_mask_6: 0.8836  loss_dice_6: 3.245  loss_ce_7: 3.346  loss_mask_7: 0.8873  loss_dice_7: 3.297  loss_ce_8: 3.423  loss_mask_8: 0.8585  loss_dice_8: 3.306    time: 0.8887  last_time: 0.7734  data_time: 0.0464  last_data_time: 0.0643   lr: 0.0001  max_mem: 23568M
[07/05 03:58:53 d2.utils.events]:  eta: 6 days, 15:40:53  iter: 399  total_loss: 77.89  loss_ce: 3.716  loss_mask: 0.8679  loss_dice: 2.889  loss_contrastive: 1.12  loss_ce_0: 3.949  loss_mask_0: 0.8357  loss_dice_0: 3.414  loss_ce_1: 3.772  loss_mask_1: 0.8641  loss_dice_1: 3.09  loss_ce_2: 3.674  loss_mask_2: 0.8774  loss_dice_2: 3.025  loss_ce_3: 3.723  loss_mask_3: 0.8584  loss_dice_3: 3.004  loss_ce_4: 3.685  loss_mask_4: 0.8478  loss_dice_4: 2.992  loss_ce_5: 3.671  loss_mask_5: 0.8197  loss_dice_5: 2.967  loss_ce_6: 3.552  loss_mask_6: 0.8053  loss_dice_6: 2.936  loss_ce_7: 3.605  loss_mask_7: 0.8678  loss_dice_7: 2.994  loss_ce_8: 3.647  loss_mask_8: 0.8313  loss_dice_8: 2.945    time: 0.8829  last_time: 0.8023  data_time: 0.0508  last_data_time: 0.0680   lr: 0.0001  max_mem: 23568M
